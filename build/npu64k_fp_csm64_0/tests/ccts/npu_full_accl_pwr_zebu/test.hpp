//===================================================
// NOTE: This file is automatically generated.
//===================================================
#include "tensor_dma.hpp"
#include "tensor_conv.hpp"
#include "npu_conv_hlapi.hpp"
#include "npu_act_lib.cpp"
#include "tensor_gtoa.cpp"
#include "tensor_gtoa_prelu.hpp"
#include "tensor_gtoa_relu1.hpp"
#include "tensor_gtoa_relu6.hpp"
#include "tensor_gtoa_unary.hpp"
#include "tensor_gtoa_binary_mm.hpp"
#include "tensor_gtoa_binary_ms.hpp"
#include "tensor_gtoa_binary_mb.hpp"
#include "tensor_gtoa_plut.hpp"
#include "tensor_gtoa_lutini.hpp"
#include "tensor_gtoa_exp.hpp"
#include "tensor_gtoa_norm.hpp"
#include "tensor_gtoa_transpose.hpp"
#ifdef SYSTEMC
#include "../../../shared/hlmodel/npu_shared_hl_mem.hpp"
#endif
using namespace tensor::v200;
using namespace npu;
#include "arcsync_utils.hpp"
#include "utils/cln_map.hpp"
#include "utils/npu_mem_utils.hpp"
#include "utils/npu_mem_map_define.hpp"
//#ifdef RTL_DPI
#include <fstream>
//#endif
#include "utils/sim_terminate.hpp"
#include "utils/arc_utils.h"

mem_alloc_t* xmalloc;
mem_alloc_t* vmalloc;
mem_alloc_t* amalloc;
mem_alloc_t* dmalloc;
mem_alloc_t* dummy_alloc;

 static ixpix_t viImg[] = {
     #include "ref/viImg.mhex"
 };
 static vyixacc_t cAcc[] = {
     #include "ref/cAcc.mhex"
 };
 static coef_t vCBA[] = {
     #include "ref/vCBA.mhex"
 };

 static vyixacc_t acc[] = {
     #include "ref/iAcc.mhex"
 };


#ifdef RTL_ARC
static ixpix_t coef_tran[] = {
    #include "rtl/vCBA.mhex"
};
#endif


#ifdef RTL_ARC
_Uncached volatile unsigned int err_cnt=0;
_Uncached volatile unsigned int chk_cnt=0;
_Uncached volatile unsigned int * err_msg= (unsigned int*)0x1f00_0000;
#else
unsigned int chk_cnt=0;
unsigned int err_cnt=0;
unsigned int * err_msg;
#endif

class test_program : public arc_program {
public:
  inline test_program() : arc_program() {
    irqflag = false;
  }
  virtual void irq() {
    irqflag = true;
  }
  // parameters to run-time
  
  gtoa_params_impl_main       rtgtoa_pars;     // main parameters
  gtoa_act_params_impl_shapes gtoa_shps;
  gtoa_params_impl_pchan<buffer_t>      pchan;      // per channel parameters encoded in VM
  gtoa_params_impl_pchan<buffer_t>*     pchan_xm;      // per channel parameters encoded in VM
  gtoa_params_impl_pchan<xbuffer_t>     pchan_xb;      // per channel parameters encoded in VM
  gtoa_params_impl_pchan<buffer_t>      pchanl1;      // per channel parameters encoded in VM
  gtoa_params_impl_pchan<xbuffer_t>     pchanl1_xb;      // per channel parameters encoded in VM
  acc_buf_impl_t<buffer_t>              ain;        // input accumulator in AM
  acc_buf_impl_t<buffer_t>*             ain_xm;
  acc_buf_impl_t<xbuffer_t>             ain_xb;
  acc_buf_impl_t<buffer_t>              aout;        // input accumulator in AM
  acc_buf_impl_t<buffer_t>*             aout_xm;
  acc_buf_impl_t<xbuffer_t>             aout_xb;

  conv_params_impl_shapes           shps;           // buffer shapes
  conv_params_impl_main             rtpars;         // main parameters
  conv_params_impl_coef<buffer_t>   impl_cf;        // encoded coefficients
  conv_params_impl_pad<buffer_t>    impl_pad;       // encoded padding
  impl_spec_container_t<buffer_t>   impl_l1buf;          // input feature-map in VM
  acc_buf_impl_t<buffer_t>          ab;             // output accumulator in AM
  conv_params_impl_coef<xbuffer_t>  impl_cf_xb;        // encoded coefficients
  conv_params_impl_pad<xbuffer_t>   impl_pad_xb;       // encoded padding
  impl_spec_container_t<xbuffer_t>  impl_l1buf_xb;          // input feature-map in VM
  acc_buf_impl_t<xbuffer_t>         ab_xb;             // output accumulator in AM
  conv_params_impl_coef<buffer_t>   impl_cfl1;        // encoded coefficients
  conv_params_impl_pad<buffer_t>    impl_padl1;       // encoded padding
  conv_params_impl_coef<xbuffer_t>  impl_cfl1_xb;        // encoded coefficients
  conv_params_impl_pad<xbuffer_t>   impl_padl1_xb;       // encoded padding
  conv_params_impl_coef<buffer_t>*  impl_cf_csm;    // encoded coefficients
  conv_params_impl_pad<buffer_t>*   impl_pad_csm;   // encoded padding
  impl_spec_container_t<buffer_t>*  impl_l1buf_csm;      // input feature-map in VM
  acc_buf_impl_t<buffer_t>*         ab_csm;         // output accumulator

  dma_params_impl_main     pr;
  tensor_t<pix_t,4,buffer_t>*       stns_csm;
  tensor_t<pix_t,4,buffer_t>*       dtns_csm;
  shape<4>                 xm_fshp{320,80,4,4};
  shape<4>                 xm_pos{0,0,0,0};
  shape<4>                 vm_fshp{320,80,4,4};
  shape<4>                 vm_pos{0,0,0,0};
#ifdef SYSTEMC
  hl_xm*                  pxm;            // pointer to xm
  hl_vm*                  pvm;            // pointer to vm
  hl_am*                  pam;            // pointer to am
  hl_dm*                  pdm;            // pointer to dm
#endif

  template<typename T> void check(T a, T b) {
    chk_cnt++;
    if (a != b) {
        *err_msg++ = err_cnt++;
        *err_msg++ = a;
        *err_msg++ = b;
        set_sim_finish_flag(err_cnt);
    }
  }

   void aot_compile() {
        int i=0;
        int k=0;
  if(NPU_SLICE_AM_SIZE==32) {
      xm_fshp = {320,8,4,4};
      vm_fshp = {320,8,4,4};
  }
        if(NPU_SLICE_FLOAT_EN) {
            conv_params<xbuffer_t> pars(1/*GRP*/,
            288/*NI*/,
            16/*NO*/,
            {16,5,1}/*OSHP*/,
            {2,1,1}/*FSHP*/,
            {1,1,1}/*SSHP*/,
            {1,1,1}/*DSHP*/,
            {16,4,287}/*PLIM*/,
            false/*UA*/,
            true/*KA*/,
            fm_cfg_fp16_e/*FMTYP*/,
            cf_cfg_fp16_e/*CFTYP*/);
        conv_params_impl_spec spc;
        pars.get_impl_params(spc);
        spc = { conv_mode_1x1i32o16,
               1,/*inn*/
               1,/*onn*/
               coef_mode_uncompressed,/*cf_mode*/
               false/*cf_4b*/};
        pars.set_impl_params(spc);
        // get the shapes of input/output tensors
        pars.get_shapes(shps);
            impl_l1buf_xb = impl_spec_container_t<xbuffer_t>(*dummy_alloc,shps.ishape);
        impl_l1buf    = impl_spec_container_t<buffer_t>(*vmalloc,shps.ishape);
        pars.init_l1(impl_l1buf_xb);
        tensor_iterator_t<ixpix_t,5,xbuffer_t> iti(impl_l1buf_xb.data);
        do {
            iti.write(viImg[i]);
            i++;
        } while(iti.next());
        xbuffer_t<fp_e5m10_t> pibuf;
        dummy_alloc->alloc(pibuf,288*1);
        tensor_t<fp_e5m10_t,1,xbuffer_t> pitns(pibuf,{(aoffset_t)288});
        tensor_iterator_t<fp_e5m10_t,1,xbuffer_t> pti(pitns);
        impl_pad_xb = conv_params_impl_pad<xbuffer_t>(*dummy_alloc, shps);
        impl_pad = conv_params_impl_pad<buffer_t>(*vmalloc, shps);
        pars.pad_enc(pitns, impl_pad_xb);
            impl_cf_xb = conv_params_impl_coef<xbuffer_t>(*dummy_alloc, shps);
        impl_cf = conv_params_impl_coef<buffer_t>(*vmalloc, impl_cf_xb.get_size());
            #ifndef RTL_FAST_SIM
        xbuffer_t<fp_e5m10_t>   cibuf;
        dummy_alloc->alloc(cibuf,288*2*1*1*16*1);
        tensor_t<fp_e5m10_t,6,xbuffer_t> citns(cibuf,{288,2,1,1,16,1});
        tensor_iterator_t<fp_e5m10_t,6,xbuffer_t> cit(citns);
        do {
           cit.write(fp_e5m10_t((uint8_t)vCBA[2*k+1],(uint8_t)vCBA[2*k]));
           k++;
        } while(cit.next());
        xbuffer_t<coef_t> czbuf;
        dummy_alloc->alloc(czbuf,47456);
        pars.coef_enc(citns, impl_cf_xb, czbuf);
        #endif
        deep_copy(impl_cf, impl_cf_xb);
        tensor_t<ixpix_t,1,buffer_t> cftns(impl_cf.cbuf,{(aoffset_t)impl_cf.cbuf.get_size()});
        tensor_iterator_t<ixpix_t,1,buffer_t> cft(cftns);
        #ifdef SYSTEMC
        //dump coef as the input initlization file for RTL simulation
        FILE *outf = NULL;
        outf= fopen("rtl/vCBA.mhex", "w");
        ixpix_t coef_tran;
        do {
            coef_tran = cft.read();
            fprintf(outf,"(ixpix_t){(pix_t)0x%02x,(pix_t)0x%02x,(pix_t)0x%02x,(pix_t)0x%02x,(pix_t)0x%02x,(pix_t)0x%02x,(pix_t)0x%02x,(pix_t)0x%02x,(pix_t)0x%02x,(pix_t)0x%02x,(pix_t)0x%02x,(pix_t)0x%02x,(pix_t)0x%02x,(pix_t)0x%02x,(pix_t)0x%02x,(pix_t)0x%02x},",coef_tran[0]&0xff,coef_tran[1]&0xff,coef_tran[2]&0xff,coef_tran[3]&0xff,coef_tran[4]&0xff,coef_tran[5]&0xff,coef_tran[6]&0xff,coef_tran[7]&0xff,coef_tran[8]&0xff,coef_tran[9]&0xff,coef_tran[10]&0xff,coef_tran[11]&0xff,coef_tran[12]&0xff,coef_tran[13]&0xff,coef_tran[14]&0xff,coef_tran[15]&0xff);
        }while(cft.next());
        fclose(outf);
        #endif
            #ifdef RTL_FAST_SIM
        set_mem_backdoor(1,0);
        do {
           cft.write(coef_tran[k]);
           k++;
        }while(cft.next());
        set_mem_backdoor(0,0);
        #endif

            ab_xb = acc_buf_impl_t<xbuffer_t>(*dummy_alloc, shps.oshape);
        tensor_t<vyixacc_t,5,xbuffer_t> ab_xb_data(ab_xb.data);
        tensor_iterator_t<vyixacc_t,5,xbuffer_t> otn(ab_xb_data);
        do {
            otn.write(cAcc[(otn.get_index(0)+1*1*otn.get_index(2)+1*2*otn.get_index(1)+1*2*5*otn.get_index(4))]);
        } while(otn.next());    // serialize run-time parameters
        pars.get_rt_params(rtpars);
        } else {
            conv_params<xbuffer_t> pars(1/*GRP*/,
            288/*NI*/,
            16/*NO*/,
            {16,5,1}/*OSHP*/,
            {2,1,1}/*FSHP*/,
            {1,1,1}/*SSHP*/,
            {1,1,1}/*DSHP*/,
            {16,4,287}/*PLIM*/,
            false/*UA*/,
            true/*KA*/,
                fm_cfg_16b_e/*FMTYP*/,
            cf_cfg_16b_e/*CFTYP*/);
        conv_params_impl_spec spc;
        pars.get_impl_params(spc);
        spc = { conv_mode_1x1i32o16,
               1,/*inn*/
               1,/*onn*/
               coef_mode_uncompressed,/*cf_mode*/
               false/*cf_4b*/};
        pars.set_impl_params(spc);
        // get the shapes of input/output tensors
        pars.get_shapes(shps);
            impl_l1buf_xb = impl_spec_container_t<xbuffer_t>(*dummy_alloc,shps.ishape);
        impl_l1buf    = impl_spec_container_t<buffer_t>(*vmalloc,shps.ishape);
        pars.init_l1(impl_l1buf_xb);
        tensor_iterator_t<ixpix_t,5,xbuffer_t> iti(impl_l1buf_xb.data);
        do {
            iti.write(viImg[i]);
            i++;
        } while(iti.next());
        xbuffer_t<pix_t> pibuf;
        dummy_alloc->alloc(pibuf,288*2);
        tensor_t<pix_t,2,xbuffer_t> pitns(pibuf,{(aoffset_t)2,(aoffset_t)288});
        tensor_iterator_t<pix_t,2,xbuffer_t> pti(pitns);
        impl_pad_xb = conv_params_impl_pad<xbuffer_t>(*dummy_alloc, shps);
        impl_pad = conv_params_impl_pad<buffer_t>(*vmalloc, shps);
        pars.pad_enc(pitns, impl_pad_xb);
            impl_cf_xb = conv_params_impl_coef<xbuffer_t>(*dummy_alloc, shps);
        impl_cf = conv_params_impl_coef<buffer_t>(*vmalloc, impl_cf_xb.get_size());
        deep_copy(impl_cf, impl_cf_xb);
        tensor_t<ixpix_t,1,buffer_t> cftns(impl_cf.cbuf,{(aoffset_t)impl_cf.cbuf.get_size()});
        tensor_iterator_t<ixpix_t,1,buffer_t> cft(cftns);
                
            #ifdef RTL_FAST_SIM
        set_mem_backdoor(1,0);
        do {
           cft.write(coef_tran[k]);
           k++;
        }while(cft.next());
        set_mem_backdoor(0,0);
        #endif

            ab_xb = acc_buf_impl_t<xbuffer_t>(*dummy_alloc, shps.oshape);
        tensor_t<vyixacc_t,5,xbuffer_t> ab_xb_data(ab_xb.data);
        tensor_iterator_t<vyixacc_t,5,xbuffer_t> otn(ab_xb_data);
        do {
            otn.write(cAcc[(otn.get_index(0)+1*1*otn.get_index(2)+1*2*otn.get_index(1)+1*2*5*otn.get_index(4))]);
        } while(otn.next());    // serialize run-time parameters
        pars.get_rt_params(rtpars);
        }

        gtoa_prelu_params<xbuffer_t> gtoa_pars(
        32/*NO*/,
        {16,5,1}/*OSHP*/,
        {1,1,1}/*OSTR*/,
        dtype_int32/*INTP*/,
        dtype_fp32/*OUTTP*/,
        false/*INASTR*/);

        gtoa_act_params_impl_spec igtoa_pars;
    igtoa_pars.onn = 1/*ONN*/;
    gtoa_pars.set_impl_params(igtoa_pars);
    gtoa_pars.get_shapes(gtoa_shps);

        ain_xb = acc_buf_impl_t<xbuffer_t>(*dummy_alloc, gtoa_shps.ishape);
        ain    = acc_buf_impl_t<buffer_t>(*amalloc,gtoa_shps.ishape);
        aout_xb= acc_buf_impl_t<xbuffer_t>(*dummy_alloc,gtoa_shps.oshape);
        aout   = acc_buf_impl_t<buffer_t>(*amalloc,gtoa_shps.oshape);
    tensor_t<vyixacc_t,5,xbuffer_t> ain_xb_data(ain_xb.data);
    tensor_iterator_t<vyixacc_t,5,xbuffer_t> aiiti(ain_xb_data);
        k = 0;
    do {
        aiiti.write(acc[k]);
                k++;
    } while(aiiti.next());

    xbuffer_t<prescale_t>        pbuf;        // per channel 2b+6b prescale for scaling down wide accumulators
    xbuffer_t<fp_e8m23_t>        bbuf;        // per channel bias [Cout]
    xbuffer_t<fp_e8m7_t>         psbuf;       // per channel positive scaling factor [Cout]
    xbuffer_t<fp_e8m7_t>         nsbuf;       // per channel negative scaling factor [Cout]
    xbuffer_t<prescale_t>        pstbuf;       // per channel shift factor [Cout]
    xbuffer_t<int8_t>            asbuf;       // per channel output quantization offset [Cout]
    tensor_t<prescale_t,1,xbuffer_t>      prescale;    // per channel 2b+6b prescale for scaling down wide accumulators
    tensor_t<fp_e8m23_t,1,xbuffer_t>      bias;        // per channel bias [Cout]
    tensor_t<fp_e8m7_t,1,xbuffer_t>       posscale;    // per channel positive scaling factor [Cout]
    tensor_t<fp_e8m7_t,1,xbuffer_t>       negscale;    // per channel negative scaling factor [Cout]
    tensor_t<prescale_t,1,xbuffer_t>      post;        // per channel postscale [Cout]
    tensor_t<int8_t,1,xbuffer_t>          asymm;       // per channel output quantization offset [Cout]
    dummy_alloc->alloc(pbuf, {32/*NO*/});
    dummy_alloc->alloc(bbuf, {32/*NO*/});
    dummy_alloc->alloc(psbuf, {32/*NO*/});
    dummy_alloc->alloc(nsbuf, {32/*NO*/});
    dummy_alloc->alloc(bbuf, {32/*NO*/});
    dummy_alloc->alloc(pstbuf, {32/*NO*/});
    dummy_alloc->alloc(asbuf, {32/*NO*/});
    prescale = tensor_t<prescale_t,1,xbuffer_t>(pbuf, {32/*NO*/});
    bias     = tensor_t<fp_e8m23_t,1,xbuffer_t>(bbuf, {32/*NO*/});
    posscale = tensor_t<fp_e8m7_t,1,xbuffer_t>(psbuf, {32/*NO*/});
    negscale = tensor_t<fp_e8m7_t,1,xbuffer_t>(nsbuf, {32/*NO*/});
    post     = tensor_t<prescale_t,1,xbuffer_t>(pstbuf, {32/*NO*/});
    asymm    = tensor_t<int8_t,1,xbuffer_t>(asbuf, {32/*NO*/});
    tensor_iterator_t<prescale_t,1,xbuffer_t> piti(prescale);
    do {
        piti.write(fp_ue6m2_t(PRESCALE_ONE));
    } while (piti.next());
    tensor_iterator_t<fp_e8m23_t,1,xbuffer_t> biti(bias);
    do {
        biti.write(fp_e8m23_t(1.0f));
    } while (biti.next());
    tensor_iterator_t<fp_e8m7_t,1,xbuffer_t> psiti(posscale);
    do {
        psiti.write(fp_e8m7_t(1.5f));
    } while (psiti.next());
    tensor_iterator_t<fp_e8m7_t,1,xbuffer_t> nsiti(negscale);
    do {
        nsiti.write(fp_e8m7_t(-0.5f));
    } while (nsiti.next());
    tensor_iterator_t<prescale_t,1,xbuffer_t> phiti(post);
    do {
        phiti.write(fp_ue6m2_t(PRESCALE_ONE));
    } while (phiti.next());
    tensor_iterator_t<int8_t,1,xbuffer_t> asiti(asymm);
    do {
        asiti.write((int8_t)21);
    } while (asiti.next());

    pchan_xb = gtoa_params_impl_pchan<xbuffer_t>(*dummy_alloc, gtoa_shps);
    gtoa_pars.param_enc(
                 prescale,
                 bias,        // per channel bias [Cout]
         posscale,    // per channel positive scaling factor [Cout]
         negscale,    // per channel negative scaling factor [Cout]
         post,        // per channel positive shift right amount [Cout]
         asymm,    // per channel negative shift right amount [Cout]
         pchan_xb        // output encoded parameters
         );

        gtoa_pars.get_rt_params(rtgtoa_pars);

        buffer_t<pix_t>                     xmbuf;
    dummy_alloc->alloc(xmbuf, get_shape_size(xm_fshp));
    tensor_t<pix_t,4,buffer_t>          xmtns(xmbuf, xm_fshp);
        tensor_iterator_t<pix_t,4,buffer_t>  idmati(xmtns);
        i = 0;
        do {
                if(i%2==0)
            idmati.write((pix_t)i);
                i++;
        } while(idmati.next());
    
    buffer_t<pix_t>                     vmbuf;
    dummy_alloc->alloc(vmbuf, get_shape_size(vm_fshp));
    tensor_t<pix_t,4>                   vmtns(vmbuf, vm_fshp);
        dma_params<buffer_t> dma_pars;
        dma_params_impl_spec ipars;
    ipars = dma_impl_idma;
    dma_pars.set_impl_params(ipars);
    dma_pars.set_src(xmtns);
    dma_pars.set_dst(vmtns);
        dma_pars.get_rt_params(pr);
  }

  void prepare_operands() {
        ab = acc_buf_impl_t<buffer_t>(*amalloc, shps.oshape);
        pchan = gtoa_params_impl_pchan<buffer_t>(*vmalloc, gtoa_shps);
        xmalloc->alloc(impl_cf_csm);
    xmalloc->alloc(impl_pad_csm);
    xmalloc->alloc(impl_l1buf_csm);
    deep_copy(impl_l1buf, impl_l1buf_xb);
    deep_copy(impl_pad, impl_pad_xb);
    mem_write(impl_cf_csm, impl_cf);
    mem_write(impl_pad_csm, impl_pad);
    mem_write(impl_l1buf_csm, impl_l1buf);
    deep_copy(ab, ab_xb);
    xmalloc->alloc(ab_csm);
    mem_write(ab_csm, ab);
        
        xmalloc->alloc(pchan_xm);
    deep_copy(pchan, pchan_xb);
    mem_write(pchan_xm, pchan);
    xmalloc->alloc(ain_xm);
    deep_copy(ain, ain_xb);
    mem_write(ain_xm, ain);
    xmalloc->alloc(aout_xm);
    deep_copy(aout, aout_xb);
    mem_write(aout_xm, aout);

        buffer_t<pix_t>          xmbuf;
    buffer_t<pix_t>          vmbuf;
    xmalloc->alloc(xmbuf, get_shape_size(xm_fshp));
    tensor_t<pix_t,4,buffer_t>                   xmtns(xmbuf, xm_fshp);
    vmalloc->alloc(vmbuf, get_shape_size(vm_fshp));
    tensor_t<pix_t,4,buffer_t>                   vmtns(vmbuf, vm_fshp);
    xmalloc->alloc(stns_csm);
    mem_write(stns_csm, xmtns);
    xmalloc->alloc(dtns_csm);
    mem_write(dtns_csm, vmtns);


  }

  void run_time() {
    // run-time execution
    npu::ctrl_dma_regs<npu_conv::conv_hlapi_t>&   rgs(*reinterpret_cast<npu::ctrl_dma_regs<npu_conv::conv_hlapi_t>*>(get_mmio_base_conv()));
    npu::ctrl_dma_regs<npu_dma::act_hlapi_t>&     activ_rgs(*reinterpret_cast<npu::ctrl_dma_regs<npu_dma::act_hlapi_t>*>(get_mmio_base_act()));
    npu::ctrl_dma_regs<npu::dma_hlapi_t>&         iregs(*reinterpret_cast<npu::ctrl_dma_regs<npu_dma::dma_hlapi_t>*>(get_mmio_base_idma()));
    npu::ctrl_dma_regs<npu::dma_hlapi_t>&         oregs(*reinterpret_cast<npu::ctrl_dma_regs<npu_dma::dma_hlapi_t>*>(get_mmio_base_odma()));
        conv_rt_list  conv_list;
        gtoa_rt_list  gtoa_list;
    conv_rt& cv(*create(*dmalloc, rtpars));
    conv_rt_impl_spec* rt;
    xmalloc->alloc(rt);
    mem_write(&rt->mmio, &rgs);  // MMIO base address
    mem_write(&rt->ctrl, (uint32_t)0x101);     // raise event at completion
    cv.set_impl_params(*rt);
    shape<3> padpos = {0,0,0};
    shape<3>* padpos_csm;
    xmalloc->alloc(padpos_csm);
    mem_write(padpos_csm, padpos);
    // initialize tile parameters
    cv.init_tile_params(*padpos_csm, *impl_cf_csm, *impl_pad_csm);
    cv.set_input(*impl_l1buf_csm);
    cv.set_output(*ab_csm);
    mmio_write(&rgs.int_enable, 0xffffffff);
        conv_list.append(&cv);
        conv_list.append(&cv);

    gtoa_rt& act_rt(*create(*dmalloc, rtgtoa_pars));
    gtoa_rt_impl_spec* rtp;
    xmalloc->alloc(rtp);
    mem_write(&rtp->mmio, &activ_rgs);
    mem_write(&rtp->ctrl, (uint32_t)0x101);
    act_rt.set_impl_params(*rtp);
    act_rt.init_tile_params(*pchan_xm);
    act_rt.set_acc_input0(*ain_xm);
        act_rt.set_acc_output(*aout_xm);
    mmio_write(&activ_rgs.int_enable, 0xffffffff);
        gtoa_list.append(&act_rt);
        gtoa_list.append(&act_rt);

        dma_rt& rt1(*create(*dmalloc, pr));
    dma_rt& rt2(*create(*dmalloc, pr));
    rt1.set_src(*stns_csm);
    rt1.set_dst(*dtns_csm);
    rt2.set_src(*dtns_csm);
    rt2.set_dst(*stns_csm);
        dma_rt_impl_spec* rtp1;
    dma_rt_impl_spec* rtp2;
    xmalloc->alloc(rtp1);
    xmalloc->alloc(rtp2);
        mem_write(&rtp1->mmio.d, &iregs);  // MMIO base address
    mem_write(&rtp1->ctrl, (uint32_t)0x101);      // raise event at completion
    rt1.set_impl_params(*rtp1);
        mmio_write(&iregs.int_enable, 0xffffffff);
    run_cycles(1);
    mem_write(&rtp2->mmio.d, &oregs);  // MMIO base address
    mem_write(&rtp2->ctrl, (uint32_t)0x101);      // raise event at completion
    rt2.set_impl_params(*rtp2);
    mmio_write(&oregs.int_enable, 0xffffffff);
    run_cycles(1);
        rt1.prepare();
        rt2.prepare();
    


    start_power();
    conv_list.execute();
    gtoa_list.execute();
        rt1.execute();
    rt2.execute();
    event_wait_all((1LL<<EVT_CONV_DONE) | (1LL<<EVT_ACT_DONE) | (1LL<<EVT_IDMA_DONE) | (1LL<<EVT_ODMA_DONE));
    stop_power();

    #ifdef SYSTEMC
    FILE *outf = NULL;
    outf= fopen("rtl/cAcc.ok", "w+");
        tensor_t<vyixacc_t,5> ab_data(ab.data);
        tensor_iterator_t<vyixacc_t,5> otn(ab_data);
    vyixacc_t out;
    #endif
    #ifndef RTL_ARC
    int count = 0;
    do {
        out = otn.read();
                for (int x=0;x<VSIZE;x++){
            for (int z=0;z<ISIZE;z++){
                if(x==0){
                    if(z==0) {
                        fprintf(outf,"0(vyixacc_t){(ixacc_t){(acc_t)0x%08x,",out[x][z]&0xffffffff);}
                    else {
                        fprintf(outf,"(acc_t)0x%08x,",out[x][z]&0xffffffff);}
                }
                else {
                    if(z==0) {
                        fprintf(outf,"},(ixacc_t){(acc_t)0x%08x,",out[x][z]&0xffffffff);}
                    else {
                        if(x==7 && z==15) {
                            fprintf(outf,"(acc_t)0x%08x,}}\n,",out[x][z]&0xffffffff);}
                        else {
                            fprintf(outf,"(acc_t)0x%08x,",out[x][z]&0xffffffff);}
                    }
                }
            }
        }
    } while(otn.next());
    set_mem_backdoor(0,0);
    #endif

        }

  void report() {
 //   ixpix_t acc[] = { 
 //       #include "rtl/cAcc.ok" 
 //   };
 //   tensor_iterator_t<ixpix_t,5> otn(l1buf.data);
 //   int k=0;
 //   do{
 //       ixpix_t real = otn.read();
 //       ixpix_t ref  = acc[k++];

 //       for(int x=0;x < 16;x++){
 //           check(ref[x], real[x]);
 //       }
 //   } while(otn.next());
    
    set_sim_finish_flag(err_cnt);

  }

  void exec() {
     evt_cfg();
#ifdef NPU_MEM_ECC
     uint32_t sfty_erp_ctrl_addr;
     sfty_erp_ctrl_addr = LOCAL_PERI_BASE + L1_SFTY_MMIO_OFFSET + 0x0_0000;
     vm_am_mem_init((uint32_t *)sfty_erp_ctrl_addr);
#endif
    vmalloc = new mem_alloc_t((uint64_t)get_slice_vm_base(), 0x100000);
    amalloc = new mem_alloc_t((uint64_t)get_slice_am_base(), 0x40000);
    dmalloc = new mem_alloc_t((uint64_t)get_fast_dccm_base(), 0x80000);
        xmalloc = new mem_alloc_t((uint64_t)0x17000000, 0x3000000);
        dummy_alloc = new mem_alloc_t((uint64_t)0x1a000000, 0x8000000);
        

    aot_compile();
    prepare_operands();
    run_time();
    report();
    arc_exit();
  }

private:
  bool irqflag;
};



